
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Network Basics &#8212; Introduction to Speech and Language Processing using Deep Learning</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"mathjax_path": "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js", "TeX": {"Macros": {"bsf": ["\\mathbb{\\mathsf{#1}}", 1]}}, "tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Sequence Modeling" href="sequence_modeling.html" />
    <link rel="prev" title="Introduction to Speech and Language Processing" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/slp_lab.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Speech and Language Processing using Deep Learning</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Introduction to Speech and Language Processing
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Network Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="sequence_modeling.html">
   Sequence Modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/neural_network_basics.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fneural_network_basics.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model">
   Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-data-set">
   Training Data Set
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-function">
   Loss Function
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient Descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">
   Stochastic Gradient Descent (SGD)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#back-propagation">
   Back-Propagation
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="neural-network-basics">
<h1>Neural Network Basics<a class="headerlink" href="#neural-network-basics" title="Permalink to this headline">¶</a></h1>
<p>There are various applications of neural-network models. In this section, for
simplicity of discussion, we will focus only on <em>the classification problem</em>.</p>
<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<p>The model can be considered a function to predicts the output class given the
input.</p>
<p>When the number of output classes is <span class="math notranslate nohighlight">\(V\)</span>, then <span class="math notranslate nohighlight">\(y\)</span> may take a value in the
following range:</p>
<div class="math notranslate nohighlight" id="equation-output-label">
<span class="eqno">(1)<a class="headerlink" href="#equation-output-label" title="Permalink to this equation">¶</a></span>\[  0 \le y \le V - 1.\]</div>
<p>In stead of the above <em>sparse representation of the output class</em>, we may use
<em>the one-hot vector</em> representation as below:</p>
</div>
<div class="section" id="training-data-set">
<h2>Training Data Set<a class="headerlink" href="#training-data-set" title="Permalink to this headline">¶</a></h2>
<p>In this section, we consider the supervised training case, which might be the
most basic case of training neural-network models.
Other cases such as unsupervised or semi-supervised training cases will
be covered later.</p>
<div class="math notranslate nohighlight" id="equation-training-set">
<span class="eqno">(2)<a class="headerlink" href="#equation-training-set" title="Permalink to this equation">¶</a></span>\[  \mathcal{T} = \big \{ (\bsf{x}^{(k)},\, \bsf{y}^{(k)})
      \; \big | \; 0 \le k \le N_{\text{tr}} - 1 \big \},\]</div>
<p>where <span class="math notranslate nohighlight">\(N_{\text{tr}}\)</span> is the number of examples in the training set
<span class="math notranslate nohighlight">\(\mathcal{T}\)</span>.</p>
</div>
<div class="section" id="loss-function">
<h2>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this headline">¶</a></h2>
<p>In training neural network models, our objective is making the model output <span class="math notranslate nohighlight">\(\hat{\bsf{y}}\)</span>
as close to the ground truth <span class="math notranslate nohighlight">\(\bsf{y}\)</span> as possible. To perform this task
analytically, we usually define a certain <em>loss function</em> and minimize it.
The entire loss <span class="math notranslate nohighlight">\(\mathbb{L}\)</span> is defined by the following equation for the
entire training set defined in <a class="reference internal" href="#equation-training-set">(2)</a>:</p>
<div class="math notranslate nohighlight" id="equation-entire-loss">
<span class="eqno">(3)<a class="headerlink" href="#equation-entire-loss" title="Permalink to this equation">¶</a></span>\[  \mathbb{L} = \frac{1}{N_{\text{tr}}} \sum_{k=0}^{N_{\text{tr} - 1}}
    \text{loss}(\bsf{y}^{(k)}, \hat{\bsf{y}}^{(k)}) \]</div>
<p>where <span class="math notranslate nohighlight">\(N_{tr}\)</span> is the number of training examples as defined in</p>
<p>Now, the question is what would be a good candidate for the loss function in
<a class="reference internal" href="#equation-entire-loss">(3)</a>. Various types of functions may be condidates for this loss
function including the well know <span class="math notranslate nohighlight">\(L_1\)</span> or <span class="math notranslate nohighlight">\(L_2\)</span> losses and so on.
As will be discussed later, an appropriate loss function is different
depending on applications.</p>
<p>In the classification problem, we would like to make the distribution of prediction by
the model <span class="math notranslate nohighlight">\(\hat{\bsf{y}} = f(\bsf{x})\)</span>
as close to the distribution of the training data set as possible. In this
classification problem, note that both the ground truth output <span class="math notranslate nohighlight">\(\bsf{y}\)</span> and
the predicted output <span class="math notranslate nohighlight">\(\hat{\bsf{y}}\)</span> are probability distributions of the set
of output labels defined in <a class="reference internal" href="#equation-output-label">(1)</a>.</p>
<p>To measure the difference between two distributions, we may use the KL
divergence or the Cross Entropy (CE). Since the distribution of the training
set is already fixed, minimizing the following Cross Entropy (CE) loss is equvalent to
minizing <em>the KL divergence</em>, which is shown in the following equation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}  \mathbb{L}_{\text{CE}} &amp; =  H(\bsf{y}, \bsf{\hat{y}}) \\
                         &amp; =  H(\bsf{y}) + D_{\text{KL}} (\bsf{y} || \hat{\bsf{y}})\end{split}\]</div>
<p>From the definition of cross entropy (CE), <span class="math notranslate nohighlight">\(\mathbb{L}_{\text{CE}}\)</span> is given by
the following equation:</p>
<div class="math notranslate nohighlight" id="equation-ce-loss">
<span class="eqno">(4)<a class="headerlink" href="#equation-ce-loss" title="Permalink to this equation">¶</a></span>\[\begin{split}  \mathbb{L}_{\text{CE}} 
         &amp; =  -E_{\bsf{y} \sim \text{training_data}} \left[ \log \hat{\bsf{y}} \right]. \\\end{split}\]</div>
<p>When the one-hot vector representation is used in <a class="reference internal" href="#equation-ce-loss">(4)</a>, <a class="reference internal" href="#equation-ce-loss">(4)</a>
is calculated as follows:</p>
<div class="math notranslate nohighlight" id="equation-ce-loss-one-hot">
<span class="eqno">(5)<a class="headerlink" href="#equation-ce-loss-one-hot" title="Permalink to this equation">¶</a></span>\[\begin{split}  \mathbb{L}_{\text{CE}} 
         &amp; =  -E_{\bsf{y} \sim \text{training_data}} 
          \left[ \sum_{v=0}^{V-1} y_v   \log \hat{y}_v  \right]. \\\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_v\)</span> and <span class="math notranslate nohighlight">\(\hat{y}_v\)</span> are <span class="math notranslate nohighlight">\(v\)</span>-th element of the one-hot vector
<span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> respectively, and <span class="math notranslate nohighlight">\(V\)</span> is the number of
output classes as shown in <a class="reference internal" href="#equation-output-label">(1)</a>.</p>
<p><a class="reference internal" href="#equation-ce-loss-one-hot">(5)</a> may be expressed in summation rather than the expection as
follows:</p>
<div class="math notranslate nohighlight" id="equation-ce-loss-one-hot-sum">
<span class="eqno">(6)<a class="headerlink" href="#equation-ce-loss-one-hot-sum" title="Permalink to this equation">¶</a></span>\[\begin{split}  \mathbb{L}_{\text{CE}} 
         &amp; =  -\sum_{k=0}^{N_{\text{tr}}}
           \sum_{v=0}^{V-1} y^{(k)}_v   \log \hat{y}^{(k)}_v . \\\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(y^{(k)}_v\)</span> and <span class="math notranslate nohighlight">\(\hat{y}^{(k)}_v\)</span> represent the <span class="math notranslate nohighlight">\(v\)</span>-th element of the
one-hot vector of the k-th training example <span class="math notranslate nohighlight">\(\mathbf{y}^{(k)}\)</span> and the
corresponding predicted output using the model <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}^{(k)}\)</span>.</p>
</div>
<div class="section" id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">¶</a></h2>
<p>Suppose that the entire parameters of our neural network model is denoted by
<span class="math notranslate nohighlight">\(\mathsf{w}\)</span>. If we can directly differentiate the loss function <span class="math notranslate nohighlight">\(\mathbb{L}\)</span>,
we improve the model parameters using the well-known Gradient Descent approach
as shown below:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cea8df4b-9b0b-4df8-9ae5-e8ca809c862d">
<span class="eqno">(7)<a class="headerlink" href="#equation-cea8df4b-9b0b-4df8-9ae5-e8ca809c862d" title="Permalink to this equation">¶</a></span>\[\begin{align}
  \bsf{w} \leftarrow  \bsf{w} - \mu \nabla_{\bsf{w}} \mathbb{L}.
\end{align}\]</div>
<p>It is well known that the direction of the negative of gradient is the
direction of <em>steepest descent</em> in the surface defined by the loss function
<span class="math notranslate nohighlight">\(\mathbb{L}\)</span>.
However, Gradient Descent (GD) is not a practical approach when the training set size is
sufficiently large for the following two reasons.</p>
<ul class="simple">
<li><p><strong>Inefficiency in computation</strong> <br>
It is usually not possible to load all the training examples in memory at
once. Thus, we may need to store partial results and part of inputs in the
disk, and repeatedly load and save partial result.</p></li>
<li><p><strong>Slow convergence</strong> <br>
Parameter update is done only once after processing the entire training
set. That is to say, the parameter update is done only once for one <em>epoch</em>.</p></li>
</ul>
</div>
<div class="section" id="stochastic-gradient-descent-sgd">
<h2>Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permalink to this headline">¶</a></h2>
<p>Unlike the GD approach using the loss from the entire training dataset, in SGD,
we update the parameter for each training example <span class="math notranslate nohighlight">\((\bsf{x}^{(k)}, \bsf{y}^{(k)})\)</span> iteratively.</p>
<div class="amsmath math notranslate nohighlight" id="equation-5ea107f0-c0a4-421e-8e96-58a2c04d2a88">
<span class="eqno">(8)<a class="headerlink" href="#equation-5ea107f0-c0a4-421e-8e96-58a2c04d2a88" title="Permalink to this equation">¶</a></span>\[\begin{align}
  \bsf{w} \leftarrow  \bsf{w} - \mu \nabla_{\bsf{w}} \mathbb{L}^{(k)}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{L}^{(k)}\)</span> is the loss value calculated using only a single
training example <span class="math notranslate nohighlight">\((\bsf{x}^{(k)}, \bsf{y}^{(k)})\)</span> and the prediction by the
model <span class="math notranslate nohighlight">\(\hat{\bsf{y}}^{(k)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[  \mathbb{L}^{(k)} = \text{loss}(\bsf{y}^{(k)}, \hat{\bsf{y}}^{(k)})\]</div>
</div>
<div class="section" id="back-propagation">
<h2>Back-Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">¶</a></h2>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">Introduction to Speech and Language Processing</a>
    <a class='right-next' id="next-link" href="sequence_modeling.html" title="next page">Sequence Modeling</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Chanwoo Kim (chanwcom@gmail.com)<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>